{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of experiments - union - DEMO\n",
    "fistly define the UUID of the experiment to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentId=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#external libraries\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as clt\n",
    "import plotly\n",
    "import plotly.subplots as sb\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import dotenv\n",
    "import pandas as pd\n",
    "import scipy.fft as fft\n",
    "import scipy.signal as sg\n",
    "import scipy.io as sio\n",
    "import pickle as pkl\n",
    "import xgboost as xgb\n",
    "import time\n",
    "import sklearn.metrics as skm\n",
    "\n",
    "#project library\n",
    "from spinco import *\n",
    "\n",
    "#project variables\n",
    "demopath=os.getcwd()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load dreams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplerate=200  #Should rethink this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals, annotations, signalsMetadata = loadDREAMSSpindlesDemo(demopath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations=annotations[annotations.type=='spindle'].reset_index(drop=True)\n",
    "len(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minDuration=0.3\n",
    "maxDuration=5\n",
    "annotations=annotations[annotations.duration>minDuration].reset_index(drop=True)\n",
    "annotations=annotations[annotations.duration<maxDuration].reset_index(drop=True)\n",
    "len(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signalsMetadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load experiment results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentModels, featureSelection = loadExperiment(experimentId,demopath+\"/DREAMS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentModels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureSelection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal hyperparameter estimation with UNION criterium\n",
    "we test the optimal points for the prediction threshold and number of boost iterations in the different validation groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentModels=experimentModels[experimentModels.criteriumName=='union'].reset_index(drop=True)\n",
    "experimentModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auxThres=pd.DataFrame({\n",
    "    'hyperThres':[0.1,0.2,0.3,0.4,0.5,0.6]\n",
    "})\n",
    "auxDepth=pd.DataFrame({\n",
    "    'hyperDepth':[10,20,30,40,50,60]\n",
    "})\n",
    "auxDuration=pd.DataFrame({\n",
    "    'hyperDuration':[0.3]\n",
    "})\n",
    "auxClose=pd.DataFrame({\n",
    "    'hyperClose':[0.1]\n",
    "})\n",
    "\n",
    "hyperParams=pd.merge(auxThres,auxDepth,how='cross')\n",
    "hyperParams=pd.merge(hyperParams,auxDuration,how='cross')\n",
    "hyperParams=pd.merge(hyperParams,auxClose,how='cross')\n",
    "\n",
    "hyperParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperExperimentModels=[]\n",
    "hyperMeanF1=[]\n",
    "hyperStdF1=[]\n",
    "\n",
    "for ind_hyper,row_hyper in hyperParams.iterrows():\n",
    "    print(ind_hyper)\n",
    "    thisExperimentModels=experimentModels.copy()\n",
    "    hyperThres=row_hyper.hyperThres\n",
    "    hyperDepth=int(row_hyper.hyperDepth)\n",
    "    hyperDuration=row_hyper.hyperDuration\n",
    "    hyperClose=row_hyper.hyperClose\n",
    "    \n",
    "    meanF1=[]\n",
    "    meanPrecision=[]\n",
    "    meanRecall=[]\n",
    "\n",
    "    stdF1=[]\n",
    "    stdPrecision=[]\n",
    "    stdRecall=[]\n",
    "\n",
    "    for ind,row in thisExperimentModels.iterrows():\n",
    "        #load model\n",
    "        model=loadBooster(row.modelId,experimentId,demopath+'/DREAMS')\n",
    "        #initialise lists\n",
    "        rawF1s=[]\n",
    "        rawPrecisions=[]\n",
    "        rawRecalls=[]\n",
    "\n",
    "        f1s=[]\n",
    "        precisions=[]\n",
    "        recalls=[]\n",
    "\n",
    "        #iterate validation subjects\n",
    "        for valSubjectId in row.val:\n",
    "            #Define annotations criterium\n",
    "            usedAnnotations=annotations[annotations.labelerId.isin(row.labelerIdList)].reset_index(drop=True)\n",
    "            #Load features and labels\n",
    "            valFeatures=loadFeatureMatrix([valSubjectId],featureSelection,signalsMetadata,samplerate,demopath)\n",
    "            valLabels=loadLabelsVector([valSubjectId],usedAnnotations,signalsMetadata,samplerate)\n",
    "            #Predict\n",
    "            valDMatrix=xgb.DMatrix(data=valFeatures)\n",
    "            probabilities=model.predict(valDMatrix,iteration_range=(0,hyperDepth))\n",
    "            raw=probabilities>=hyperThres\n",
    "            #Processed labels\n",
    "            processed=labelingProcess(raw,hyperClose,hyperDuration,samplerate)\n",
    "            gtAnnotations=labelVectorToAnnotations(valLabels,samplerate)\n",
    "            detections=labelVectorToAnnotations(processed,samplerate)\n",
    "            #Metrics\n",
    "            f,r,p=annotationPairToMetrics(gtAnnotations,detections)\n",
    "            \n",
    "            #Metric appends\n",
    "            f1s.append(f)\n",
    "            precisions.append(p)\n",
    "            recalls.append(r)\n",
    "\n",
    "        #statistics of the metrics over the subjects of the validation set\n",
    "        meanF1.append(np.mean(f1s))\n",
    "        meanPrecision.append(np.mean(precisions))\n",
    "        meanRecall.append(np.mean(recalls))\n",
    "\n",
    "        stdF1.append(np.std(f1s))\n",
    "        stdPrecision.append(np.std(precisions))\n",
    "        stdRecall.append(np.std(recalls))\n",
    "        \n",
    "    thisExperimentModels['meanF1']=meanF1\n",
    "    thisExperimentModels['meanPrecision']=meanPrecision\n",
    "    thisExperimentModels['meanRecall']=meanRecall\n",
    "    \n",
    "    thisExperimentModels['stdF1']=stdF1\n",
    "    thisExperimentModels['stdPrecision']=stdPrecision\n",
    "    thisExperimentModels['stdRecall']=stdRecall\n",
    "\n",
    "    hyperExperimentModels.append(thisExperimentModels)\n",
    "    hyperMeanF1.append(np.mean(thisExperimentModels['meanF1']))\n",
    "    hyperStdF1.append(np.std(thisExperimentModels['meanF1']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperParams['meanMeanF1']=hyperMeanF1\n",
    "hyperParams['stdMeanF1']=hyperStdF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimalInd=hyperParams[hyperParams.meanMeanF1==np.max(hyperParams.meanMeanF1)].index[0]\n",
    "print(\"maximal mean score at:\")\n",
    "optimal=hyperParams.iloc[optimalInd]\n",
    "optimal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphical representation of the optimal point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux=hyperParams[(hyperParams.hyperDepth==optimal.hyperDepth)&(hyperParams.hyperDuration==optimal.hyperDuration)&\n",
    "                (hyperParams.hyperClose==optimal.hyperClose)].reset_index(drop=True)\n",
    "px.scatter(aux,x='hyperThres',y='meanMeanF1',error_y='stdMeanF1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux=hyperParams[(hyperParams.hyperThres==optimal.hyperThres)&(hyperParams.hyperDuration==optimal.hyperDuration)&\n",
    "                (hyperParams.hyperClose==optimal.hyperClose)].reset_index(drop=True)\n",
    "px.scatter(aux,x='hyperDepth',y='meanMeanF1',error_y='stdMeanF1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "69eb08d3e713eff25484e90b18c01e15df779e80a0db295f9ddc53faed0455be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
