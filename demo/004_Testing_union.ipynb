{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing for UNION - DEMO\n",
    "fistly define the UUID of the experiment to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentId=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#external libraries\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as clt\n",
    "import plotly\n",
    "import plotly.subplots as sb\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import dotenv\n",
    "import pandas as pd\n",
    "import scipy.fft as fft\n",
    "import scipy.signal as sg\n",
    "import scipy.io as sio\n",
    "import pickle as pkl\n",
    "import xgboost as xgb\n",
    "import time\n",
    "import sklearn.metrics as skm\n",
    "\n",
    "#project library\n",
    "from spinco import *\n",
    "\n",
    "#project variables\n",
    "demopath=os.getcwd()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load dreams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplerate=200  #Should rethink this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals, annotations, signalsMetadata = loadDREAMSSpindlesDemo(demopath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signalsMetadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load experiment results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentModels, featureSelection = loadExperiment(experimentId,demopath+'\\DREAMS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we show the difference in class inbalance for the annotation criteria considered\n",
    "experimentModels[['criteriumName','spindleTimeRate']].groupby('criteriumName').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureSelection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter definition\n",
    "this should come from a previous evaluation notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperClose=0.1\n",
    "hyperDuration=0.3\n",
    "hyperThres=0.3\n",
    "hyperDepth=40"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with UNION criterium\n",
    "we test the optimal points for the prediction threshold and number of boost iterations in the different validation groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentModels=experimentModels[experimentModels.criteriumName=='union'].reset_index(drop=True)\n",
    "experimentModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawF1s=[]\n",
    "rawPrecisions=[]\n",
    "rawRecalls=[]\n",
    "\n",
    "f1s=[]\n",
    "precisions=[]\n",
    "recalls=[]\n",
    "\n",
    "eventF1s=[]\n",
    "eventPrecisions=[]\n",
    "eventRecalls=[]\n",
    "\n",
    "checks=[]\n",
    "\n",
    "for ind,row in experimentModels.iterrows():\n",
    "    print('*************************')\n",
    "    print(str(ind+1)+' of '+str(len(experimentModels)) )\n",
    "    #load model\n",
    "    model=loadBooster(row.modelId,experimentId,demopath+'/DREAMS')\n",
    "\n",
    "    testSubjectId=row.test\n",
    "    #Define annotations criterium\n",
    "    usedAnnotations=annotations[annotations.labelerId.isin(row.labelerIdList)].reset_index(drop=True)\n",
    "    #Load features and labels\n",
    "    testFeatures=loadFeatureMatrix([testSubjectId],featureSelection,signalsMetadata,samplerate,demopath)\n",
    "    testLabels=loadLabelsVector([testSubjectId],usedAnnotations,signalsMetadata,samplerate)\n",
    "\n",
    "    #Predict\n",
    "    testDMatrix=xgb.DMatrix(data=testFeatures)\n",
    "    probabilities=model.predict(testDMatrix,iteration_range=(0,hyperDepth))\n",
    "    rawLabels=probabilities>=hyperThres\n",
    "    #Raw Metrics\n",
    "    rawTp=np.sum(rawLabels*testLabels)\n",
    "    rawFp=np.sum(rawLabels*(1-testLabels))\n",
    "    rawTn=np.sum((1-rawLabels)*(1-testLabels))\n",
    "    rawFn=np.sum((1-rawLabels)*testLabels)\n",
    "    #Raw appends\n",
    "    rawF1s.append(2*rawTp/(2*rawTp+rawFp+rawFn))\n",
    "    rawPrecisions.append(rawTp/(rawTp+rawFp) )\n",
    "    rawRecalls.append(rawTp/(rawTp+rawFn))\n",
    "    #Process\n",
    "    processedLabels=labelingProcess(rawLabels,hyperClose,hyperDuration,samplerate)\n",
    "    #Processed metrics\n",
    "    tp=np.sum(processedLabels*testLabels)\n",
    "    fp=np.sum(processedLabels*(1-testLabels))\n",
    "    tn=np.sum((1-processedLabels)*(1-testLabels))\n",
    "    fn=np.sum((1-processedLabels)*testLabels)\n",
    "    #Processed appends\n",
    "    f1s.append(2*tp/(2*tp+fp+fn))\n",
    "    precisions.append(tp/(tp+fp))\n",
    "    recalls.append(tp/(tp+fn))\n",
    "\n",
    "    #By-event metrics\n",
    "    processedAnnotations=labelVectorToAnnotations(processedLabels,samplerate)\n",
    "    gtAnnotations=labelVectorToAnnotations(testLabels,samplerate)   #<- or just filter the annotations\n",
    "    f,r,p=annotationPairToMetrics(gtAnnotations,processedAnnotations)\n",
    "    \n",
    "    #calculate metrics\n",
    "    eventF1s.append(f)\n",
    "    eventPrecisions.append(p)\n",
    "    eventRecalls.append(r)\n",
    "\n",
    "#include metrics in the dataframe\n",
    "experimentModels['rawF1']=rawF1s\n",
    "experimentModels['rawPrecision']=rawPrecisions\n",
    "experimentModels['rawRecall']=rawRecalls\n",
    "\n",
    "experimentModels['f1']=f1s\n",
    "experimentModels['precision']=precisions\n",
    "experimentModels['recall']=recalls\n",
    "\n",
    "experimentModels['eventF1']=eventF1s\n",
    "experimentModels['eventPrecision']=eventPrecisions\n",
    "experimentModels['eventRecall']=eventRecalls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=px.scatter(experimentModels,x='rawF1',y='f1',color='test',hover_name='modelId', marginal_y=\"histogram\")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=experimentModels['rawF1'], y=experimentModels['rawF1'], name=\"identity\", mode='lines',fill=\"toself\")\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=px.scatter(experimentModels,x='rawF1',y='eventF1',color='test',hover_name='modelId', marginal_y=\"histogram\")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=experimentModels['rawF1'], y=experimentModels['rawF1'], name=\"identity\", mode='lines',fill=\"toself\")\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=px.scatter(experimentModels,x='eventF1',y='eventPrecision',color='test',hover_name='modelId', marginal_y=\"histogram\")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=experimentModels['eventF1'], y=experimentModels['eventF1'], name=\"identity\", mode='lines',fill=\"toself\")\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=px.scatter(experimentModels,x='eventF1',y='eventRecall',color='test',hover_name='modelId', marginal_y=\"histogram\")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=experimentModels['eventF1'], y=experimentModels['eventF1'], name=\"identity\", mode='lines',fill=\"toself\")\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentModels[['test','eventF1','eventPrecision','eventRecall']].groupby('test').describe(percentiles=[0.5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### event metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentModels[['test','eventF1','eventPrecision','eventRecall']].groupby('test',as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentModels[['test','eventF1','eventPrecision','eventRecall']].groupby('test',as_index=True).mean().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentModels[['test','eventF1','eventPrecision','eventRecall']].groupby('test',as_index=True).mean().std()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sample metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentModels[['test','f1','precision','recall']].groupby('test',as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentModels[['test','f1','precision','recall']].groupby('test',as_index=True).mean().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentModels[['test','f1','precision','recall']].groupby('test',as_index=True).mean().std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auxPrecision=pd.DataFrame({\n",
    "    'metric':'event precision',\n",
    "    'value':experimentModels.eventPrecision,\n",
    "    'event F1':experimentModels.eventF1\n",
    "})\n",
    "\n",
    "auxRecall=pd.DataFrame({\n",
    "    'metric':'event recall',\n",
    "    'value':experimentModels.eventRecall,\n",
    "    'event F1':experimentModels.eventF1\n",
    "})\n",
    "visualTradeoff=pd.concat((auxPrecision,auxRecall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=px.scatter(visualTradeoff,x='event F1',y='value',color='metric', marginal_y=\"histogram\")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=experimentModels['eventF1'], y=experimentModels['eventF1'], name=\"identity\", mode='lines',fill=\"toself\")\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CODA: by-event evaluation summary of the last fold testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotationPairToGraph(gtAnnotations,processedAnnotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "congratulations for reaching this point, enjoy!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "69eb08d3e713eff25484e90b18c01e15df779e80a0db295f9ddc53faed0455be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
